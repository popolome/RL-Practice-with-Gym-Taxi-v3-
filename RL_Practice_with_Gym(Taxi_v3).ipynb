{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVCyry3P9WO+PiEAFkgFCC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/popolome/RL-Practice-with-Gym-Taxi-v3-/blob/main/RL_Practice_with_Gym(Taxi_v3).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dbk8BuZSzgIS",
        "outputId": "9deeabb2-4250-475b-a154-858ddaafb1dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "bQDxpVnxzxtl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Create the environment"
      ],
      "metadata": {
        "id": "5e-dYx9l1QpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Taxi-v3', render_mode='ansi')   # This ANSI is for text rendering\n",
        "num_states = env.observation_space.n    # This is 500 states\n",
        "num_actions = env.action_space.n    # This is 6 actions"
      ],
      "metadata": {
        "id": "1ahJc3_Dz_5-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Initialize the Q-table"
      ],
      "metadata": {
        "id": "dN0ZfPoF1iFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q = np.zeros((num_states, num_actions), dtype = np.float32) # To define the Q matrix of zeros"
      ],
      "metadata": {
        "id": "tXTqqbqb0XNW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Define the hyperparameters"
      ],
      "metadata": {
        "id": "8U58lKek1prn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.1   # This is the learning rate\n",
        "gamma = 0.9  # This is the\n",
        "epsilon = 1.0   # This is the exploration probability\n",
        "min_epsilon = 0.01\n",
        "epsilon_decay = 0.995\n",
        "num_episodes = 5000\n",
        "max_steps = 100   # This is the optional truncation per episode"
      ],
      "metadata": {
        "id": "WZb_Ird305nd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Training Loop"
      ],
      "metadata": {
        "id": "gHPgNnHn2ryc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(num_episodes):\n",
        "  state, _ = env.reset()\n",
        "  done = False\n",
        "  step = 0\n",
        "\n",
        "  while not done and step < max_steps:    # Epsilon-greedy action selection\n",
        "    if np.random.rand() < epsilon:\n",
        "      action = env.action_space.sample()    # Exploration\n",
        "    else:\n",
        "      action = np.argmax(Q[state])    # Exploitation\n",
        "\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)   # Take a step/action\n",
        "    done = terminated or truncated\n",
        "\n",
        "    # Q-learning update\n",
        "    Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
        "\n",
        "    state = next_state    # Update current state to next_state and repeat\n",
        "    step += 1     # Increment step by 1 ensuring less than 100(max_steps)\n",
        "\n",
        "epsilon = max(min_epsilon, epsilon * epsilon_decay)     # Decay the epsilon every step\n",
        "\n",
        "if (episode + 1) % 500 == 0:\n",
        "  print(f\"Episode {episode + 1}/{num_episodes} complete\")   # Print the progress"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwJ1bcBe2HLQ",
        "outputId": "5e9608f2-1b2e-4b53-f9b4-42d1ef3e8abc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 5000/5000 complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 - Test the learned policy"
      ],
      "metadata": {
        "id": "VQdb0kuu7T38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state, _ = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "print(\"\\n--- Testing the learned policy ---\\n\")\n",
        "\n",
        "while not done:\n",
        "  action = np.argmax(Q[state])      # Choose the best action at state\n",
        "  next_state, reward, terminated, truncated, info = env.step(action)\n",
        "  done = terminated or truncated\n",
        "  state = next_state\n",
        "  total_reward += reward      # Updates total_reward with the immediate reward\n",
        "  print(env.render())     # Print the text map of Taxi-v3\n",
        "\n",
        "print(f\"\\nTotal Reward with learned policy: {total_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOuMS4ld6Bl6",
        "outputId": "a36863dd-bc3d-4f9b-a8dc-0c2288fa533e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing the learned policy ---\n",
            "\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[42mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "\n",
            "Total Reward with learned policy: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jr3bCTze8k_N"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}